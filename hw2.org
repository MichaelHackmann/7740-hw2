
#+title: Homework 2: Formal Languages, Parsing, and Semantics
#+author: Toni Kazic
#+date: Fall, 2024


# revised <2021-09-25 Sat>

#+SETUPFILE: "../../../common/preamble.org"
#+LATEX_CLASS: article
#+OPTIONS: toc:nil
#+OPTIONS: ^:nil

#+LATEX_HEADER: \usepackage{langsci-avm}
# http://ftp.math.purdue.edu/mirrors/ctan.org/macros/latex/contrib/langsci-avm/langsci-avm.pdf
# and see also
# https://userblogs.fu-berlin.de/langsci-press/2020/04/20/writing-avms-easily-in-latex-the-new-langsci-avm-package/


#+LATEX_HEADER: \newcommand{\grmr}[2]{\ensuremath{\mathrm{#1} & \,\longrightarrow\, \mathrm{#2}}}
#+LATEX_HEADER: \newcommand{\txtgrmr}[2]{\ensuremath{\mathrm{#1} \,\longrightarrow\, \mathrm{#2}}}
#+LATEX_HEADER: \newcommand{\grmrhs}[1]{\ensuremath{& \,\longrightarrow\, \mathrm{#1} }}
#+LATEX_HEADER: \newcommand{\wa}[1]{\type{\textnormal{\w{#1}}}}

# compile with pdflatex
#
# Kazic, 3.11.2020

 

* Introduction

In this homework, the syntactic and semantic rubber hits the road.  This
homework introduces the deeper structures of language, especially when
phrased formally; looks at syntax and parsing; and extends the notion of
parsing to semantics.



* Who's Who and Solution Patterns
<<whoswho>>


** Lead Person:  yellow


** Group Members

| first name last name | color                                |
|----------------------+--------------------------------------|
| Michael Hackmann     | yellow \color{yellow}\rule{5mm}{3mm} |
|                      | green \color{green}\rule{5mm}{3mm}   |
| Ethan Glass          | purple \color{violet}\rule{5mm}{3mm} |


** Three Member Solution Patterns

$i$ is the question number.

#+begin_center
#+ATTR_LaTeX: :mode inline-math :environment array
| \text{color}                         | \text{draft solution} | \text{revise solution} |
|--------------------------------------+-----------------------+------------------------|
| green \color{green}\rule{5mm}{3mm}   | i \mod 3 = 1          | i \mod 3 = 0           |
| yellow \color{yellow}\rule{5mm}{3mm} | i \mod 3 = 2          | i \mod 3 = 1           |
| purple \color{violet}\rule{5mm}{3mm} | i \mod 3 = 0          | i \mod 3 = 2           |
#+end_center


** Two Member Solution Patterns

| color                                | draft solution | revise solution |
|--------------------------------------+----------------+-----------------|
| green \color{green}\rule{5mm}{3mm}   | odds           | evens           |
| yellow \color{yellow}\rule{5mm}{3mm} | evens          | odds            |




* General Instructions


   + /Fill out the group members table and follow the solution patterns/ in
     Section [[whoswho]].

   + /If the question is unclear, tell me your interpretation of it as part
     of your answer./  Feel free to ask about the questions in class or on
     the Slack channel (use =@channel= as others will probably be puzzled
     too). 

   + /For questions using corpora, use the corpus of the lead person./

   + /Put your draft answers right after each question using a *complete,
     functional* =org= mode code or example block./ Make sure your code
     block is complete and functional by testing it in your copy of this
     homework file.

   + /Each group member reviews the others' draft solutions and you revise them together/.

   + /Discuss each other's draft and reviews, finalizing the answers./

   + /Show all your work: code, results, and analysis./  Does your code
     work in this file and produce *exactly* the results you show? 

   + /Post the completed file to Canvas no later than noon on the Tuesday
     indicated/ in the [[../syllabus.org::schedule][schedule in the syllabus]], naming your file with each
     person's first name (no spaces in the file name, and don't forget the
     =.org= extension!).  Only one person should submit the final file.


* Hints


** Make sure the structure of the grammar can be parsed by the parser.

For example, a recursive descent parser cannot terminate the parse of a
left-recursive grammar.



** Use re-entrancy if you need it.

=NLTK= has some notation for this.




* Questions

# added directions on proving <2023-10-20 Fri>
# clarified that the rules are from separate grammars <2023-12-11 Mon>
1. [@1] <<prod-rules>> Remember those silly tags from [[file:./hw1.org::silly-tags][hw1.org]]?  Let
#
#
\begin{align}
N &= \{ \textrm{FOO,BAR,EGO,NEED,ADS,DUCK,MANSE} \} \ \text{and} \nonumber \\
T &= \{ \w{dog, black, racing, was, squirrel, tree, burrow, ground hog, bushes, towards,
hunting, back, wee} \}  \nonumber
\end{align}
#
For each of the following production rules, state from which class of
language they come and /show why/ by derivation from the language
definitions (that is, write the proof and describe it).
   + rule 1 :: $\txtgrmr{FOO}{EGO \ NEED \ DUCK}$
   + rule 2 :: $\txtgrmr{FOO \ DUCK}{EGO \ NEED \ DUCK}$ 
   + rule 3 :: $\txtgrmr{FOO}{EGO \ \w{dog} \ DUCK}$  
   + rule 4 :: $\txtgrmr{FOO \ \w{ground hog}}{EGO \ \w{dog} \ DUCK \ \w{squirrel}}$  
   + rule 5 :: $\txtgrmr{FOO}{\w{black} \ \w{dog} \ DUCK}$  
#
Each of the rules is abstracted from a different grammar!

Language definitions:

Type 0: Recursively-Enumerable - α -> β, where where α and β are strings of terminals
and non-terminals, and α is non-empty.

Type 1: Context-Sensitive - αAβ -> αγβ, where A is a non-terminal, α, β are
any strings of terminals and non-terminals, and γ is a non-empty string.
Importantly, the length of the left-hand side should not exceed the length
of the right-hand side.

Type 2: Context-Free - A -> γ, where A is a single non terminal and γ is
any string of terminals and non-terminals.

Type 3:  A -> aB or A -> aB, where A and B are non-terminals, and a is a
terminal. These rules must produce either a terminal symbol followed by a
non-terminal or just a terminal. 

+ rule 1 is context-free, because A is a single non terminal (FOO) while
  γ, the string of terminals and non-terminals, is EGO\NEED\DUCK.

+ rule 2 is context-sensitive, because A represents FOO, α represents
  DUCK, β represents a terminal, and γ represents the string EGO \ NEED.
   
+ rule 3 is context-free, since the lefthand side is a single non-terminal
  (FOO) and the right is a combo of terminals (\wdog) and non-terminals
  (EGO DUCK).
   
+ rule 4 is context-sensitive, because the left side containts two
  elements, and the right hand side meets the constraints. Crucially, the
  left-hand side does not exceed the length of the right.
   
+ rule 5 is context-free, becauset the left-hand side is a single
  non-terminal while the right is a string of terminals and non-terminals. 


2. [@2] <<which-recursn>> Consider the following grammar.
#+BEGIN_EXAMPLE
N = {A,B,C,D}
T = {foo,bar}
S = {C}
P = {C -> A B
     B -> A D
     B -> A
     D -> A A
     A -> T
     }
#+END_EXAMPLE
Is the grammar left-, right-, neither-, or both-recursive?  Why?

+ I believe the grammar is neither-recursive, because in the production
  rules none of of the non-terminals reappear on the lefthand and righthand
  sides. There is no recursive outcome. 


3. [@3] <<manual-parse>> By hand, generate a construct for each unique
   length of output using the grammar of question [[which-recursn]] and show
   them and their derivation as an =org= table:

| sentence        | rule sequence and comments                                |
|-----------------+-----------------------------------------------------------|
| foo bar |


# clarified they are not to write more complex productn rules, 
# that all words are to be filtered at once, and that no trivial
# regexes are to be used.
#
# <2023-12-11 Mon>
4. [@4] <<regex>> For the terminals in question [[prod-rules]], write the
   minimum number of Python *regular expressions* (/not production rules
   from context-free or greater grammars!/) to distinguish among them /when
   the entire group of words is presented/ (not one-by-one!).  Do not use
   trivial regexes that match one and only one word.  Use conditionals and
   order the regexs into a tree until all terminals are recognized without
   ambiguities.  Carry your tree out until each terminal has a regular
   expression that places it in the leaves.  Include a sketch of your tree
   if you think it will help!

   Start
   |
   |---^EGO
   |    |---NEED -> rule 1/2
   |    |    |___DUCK$ -> rule 1/2
   |    |---DOG -> rule 3/4
   |    |    |___squirrel$ -> rule 4
   |    |    |___DUCK$ -> rule 3
   |    |
   |___^black
        |___dog DUCK$ -> rule 5


As you can see, this tree identifies all the terminals. Because all the
strings start with either EGO or black, those begin the tree branches. From
there they all branch off either to NEED or to DOG, and in the case of
"black" it could branch off to "dog DUCK". So by following the branches of
the tree based off the string in question, you will ifnd out what rule of
the prod rules it conforms to. The actual regex's are in the below python
code of question 5. 


5. [@5] <<regex-imp>> Now implement your regular expression tree in
   question [[regex]] and show the code and results.

#+begin_src python :results output
import re

# define regex patterns for each rule
patterns = {
    "Rule 1/2": r'^EGO NEED DUCK$',       # matches both Rule 1 and Rule 2
    "Rule 3": r'^EGO dog DUCK$',           # matches Rule 3
    "Rule 4": r'^EGO dog DUCK squirrel$',  # matches Rule 4
    "Rule 5": r'^black dog DUCK$',         # matches Rule 5
}

# function to match input against the regex tree
def match_rule(sequence):
    for rule, pattern in patterns.items():
        if re.match(pattern, sequence):
            return rule
    return "No match"

# test cases
test_sequences = [
    "EGO NEED DUCK",         # rule 1/2
    "EGO dog DUCK",          # rule 3
    "EGO dog DUCK squirrel", # rule 4
    "black dog DUCK",        # rule 5
    "EGO NEED DUCK"          # rule 1/2
]

# running the matcher and showing results
results = {seq: match_rule(seq) for seq in test_sequences}

# display results
for seq, result in results.items():
    print(f"Sequence: '{seq}' -> {result}")

#+end_src

#+results:
: Sequence: 'EGO NEED DUCK' -> Rule 1/2
: Sequence: 'EGO dog DUCK' -> Rule 3
: Sequence: 'EGO dog DUCK squirrel' -> Rule 4
: Sequence: 'black dog DUCK' -> Rule 5


6.  [@6] <<first-gram>> Write a grammar that captures the following sentences:
#
   + sentence 1 :: ``The cheerful black dog slept quietly by the chair.''
   + sentence 2 :: ``A sleepy yellow dog stretched his back.''
   + sentence 3 :: ``Somebody downstairs made the coffee.''
#
Put the phrases generated by each rule from these sentences alongside the
rules, again as an =org= table (this example is *JUST to illustrate format,
it is not correct!*):

RULE 1: S (sentence) -> NP VP
RULE 2: NP (noun phrase) -> D A | D A A N | N | D N
RULE 3: VP (verb phrase) -> V NP | V AV PP
RULE 4: PP (prep phrase) -> P NP
RULE 5: D (determiner) -> "the" | "The" | "A" | "Somebody" | "Some" | "His"
RULE 6: A (adjective) -> "cheerful" | "sleepy" | "yellow" | "black" | "downstairs"
RULE 7: N (noun) -> "dog" | "chair" | "back" | "coffee"
RULE 8: V (verb) -> "slept" | "stretched" | "made"
RULE 9: AV (adverb) -> "quietly" 
RULE 9: P (prep) -> "by"
#
| rule                | phrase                                                                                                                                 |   |
|---------------------+----------------------------------------------------------------------------------------------------------------------------------------+---|
| S $\rightarrow$ A B | ( foo bar barbar )                                                                                                                     |   |
| S > NP VP           | ( The cheerful black dog slept quietly by the chair )( A sleepy yellow dog stretched his back )( Somebody downstairs made the coffee ) |   |
| NP > D A            | ( Somebody downstairs )                                                                                                                |   |
| NP > D A A N        | ( The cheerful black dog ) ( A sleepy yellow dog )                                                                                     |   |
| VP > V NP           | ( stretched his back) (made the coffee)                                                                                                |   |
| VP > V AV PP        | ( slept quietly by the chair)                                                                                                          |   |
| PP > P NP           | ( by the chair)                                                                                                                        |   |
| D                   | ( the )( The )( A )( Somebody )( his )                                                                                                 |   |
| A                   | ( cheerful )( sleepy )( yellow )( black )( downstairs )                                                                                |   |
| N                   | ( dog )( chair )( back )( coffee )                                                                                                     |   |
| V                   | ( slept )( stretched )( made )                                                                                                         |   |
| P                   | ( by )                                                                                                                                 |   |

# added pretty printing, so much easier to follow
#
# Kazic, 15.12.2023


7. [@7] <<recur-desc-parser>> Now implement the grammar of question
   [[first-gram]] as a recursive descent parser.  Parse each sentence, showing
   the results as a prettily printed tree, and compare them.  What do you
   observe?

#+begin_src python :results output
import nltk
from nltk.tree import Tree

# define the grammar rules
grammar_rules = {
    'S': [['NP', 'VP']],
    'NP': [['D', 'A'], ['D', 'A', 'A', 'N'], ['N'], ['D', 'N']],
    'VP': [['V', 'NP'], ['V', 'AV', 'PP']],
    'PP': [['P', 'NP']],
    'D': ['the', 'The', 'A', 'Somebody', 'Some', 'His'],
    'A': ['cheerful', 'sleepy', 'yellow', 'black', 'downstairs'],
    'N': ['dog', 'chair', 'back', 'coffee'],
    'V': ['slept', 'stretched', 'made'],
    'AV': ['quietly'],
    'P': ['by'],
}

#+end_src




8. <<chart-parser>> Following on, implement the grammar of question
   [[first-gram]] as a chart parser.  Parse each sentence, showing the results,
   and compare these chart parsing results to your results in question
   [[recur-desc-parser]].  What do you observe?

#+begin_src python :results output
  import nltk
  from nltk import CFG

  # define the grammar
  grammar = CFG.fromstring("""
    S -> NP VP
    NP -> D A | D A A N | D N
    VP -> V NP | V AV P NP
    PP -> P NP
    D -> 'the' | 'The' | 'A' | 'Somebody' | 'Some' | 'his'
    A -> 'cheerful' | 'sleepy' | 'yellow' | 'black' | 'downstairs'
    N -> 'dog' | 'chair' | 'back' | 'coffee'
    V -> 'slept' | 'stretched' | 'made'
    AV -> 'quietly'
    P -> 'by'
  """)

  # create a chart parser
  chart_parser = nltk.ChartParser(grammar)

  # sentences to parse
  sentences = [
    "The cheerful black dog slept quietly by the chair",
    "A sleepy yellow dog stretched his back",
    "Somebody downstairs made the coffee"
  ]

  # parsing and displaying trees
  for sentence in sentences:
    print(f"Parsing sentence: '{sentence}'")
    tokens = sentence.split()
    parse_trees = list(chart_parser.parse(tokens))  # Get all possible parse trees
    for tree in parse_trees:
      print(tree)
      tree.pretty_print()
    if not parse_trees:
      print("No parse trees found.")
    print()


#+end_src

#+results:
#+begin_example
Parsing sentence: 'The cheerful black dog slept quietly by the chair'
(S
  (NP (D The) (A cheerful) (A black) (N dog))
  (VP (V slept) (AV quietly) (P by) (NP (D the) (N chair))))
                         S                            
        _________________|____________                 
       |                              VP              
       |                  ____________|_______         
       NP                |      |     |       NP      
  _____|____________     |      |     |    ___|____    
 D     A       A    N    V      AV    P   D        N  
 |     |       |    |    |      |     |   |        |   
The cheerful black dog slept quietly  by the     chair


Parsing sentence: 'A sleepy yellow dog stretched his back'
(S
  (NP (D A) (A sleepy) (A yellow) (N dog))
  (VP (V stretched) (NP (D his) (N back))))
                   S                        
       ____________|_____________            
      |                          VP         
      |                    ______|___        
      NP                  |          NP     
  ____|____________       |       ___|___    
 D    A      A     N      V      D       N  
 |    |      |     |      |      |       |   
 A  sleepy yellow dog stretched his     back


Parsing sentence: 'Somebody downstairs made the coffee'
(S
  (NP (D Somebody) (A downstairs))
  (VP (V made) (NP (D the) (N coffee))))
                         S                 
           ______________|____              
          |                   VP           
          |               ____|___          
          NP             |        NP       
    ______|______        |     ___|____     
   D             A       V    D        N   
   |             |       |    |        |    
Somebody     downstairs made the     coffee


#+end_example


9. <<clock>> Extend your grammar for the sentences in question
   [[recur-desc-parser]]  so that it can parse sentences 4--6 below.  Time the
   implementation's performance for each sentence, doing this 1000 times
   for each sentence for better estimates, and put the results  in an =org= table.
   + sentence 4 :: ``We had a long walk to the park and Vinny played with
     three other dogs.''
   + sentence 5 :: ``It was sunny today but might not be tomorrow.''
   + sentence 6 :: ``There are 49 angels dancing on the head of this pin.''

#+begin_src python :results output
grammar = CFG.fromstring("""
    S -> NP VP | S CONJ S 
    NP -> D A | D A A N | D N | N | D N P NP | NUM N | N PP | D N PP | D A N
    VP -> V NP | V AV P NP | V NP PP | V ADVP  
    PP -> P NP
    ADVP -> AV | AV ADVP 
    D -> 'The' | 'the' | 'a' | 'A' | 'Somebody' | 'Some' | 'his' | 'We' | 'It' | 'There' | 'this' 
    A -> 'cheerful' | 'sleepy' | 'other' | 'yellow' | 'black' | 'downstairs' | 'long' | 'sunny'  
    N -> 'dog' | 'pin.' | 'head' | 'dogs.' | 'Vinny' | 'chair' | 'back' | 'coffee' | 'walk' | 'park' | 'angels' | 'pin' 
    V -> 'was' | 'might' | 'are' | 'be' | 'slept' | 'stretched' | 'made' | 'had' | 'played' | 'dancing'  
    AV -> 'not' | 'tomorrow.' | 'quietly' | 'today' | 'tomorrow' 
    P -> 'by' | 'of' | 'to' | 'on' | 'with'  
    NUM -> '49' | 'three' 
    CONJ -> 'and' | 'but'  
""")

#+end_src



10. <<avm-graph>> Consider this attribute-value matrix:
#
#+begin_export latex
\begin{center}
\avm{
[ CAT  & s \\
  HEAD & [ AGR   & \1 [ NUM & sg \\
                        PER & 3 ] \\
           SUBJ  & [ AGR \1 ] ] ] \\
}.
\end{center}
#+end_export
#
Draw the corresponding directed acyclic graph, ideally in Python.  (A hand-drawn figure is fine:
just photograph it and include the image below, as is done in [[file:../notes.org][notes.org]].)

11. [@11] <<basic-fea-struc>> Now extend your grammar from question [[clock]] to include features
    relevant to subject-verb agreement, using =nltk.FeatStruct()= from
    chapter nine, so that you can parse sentences 1--9.  Using
    =cp.parse()=, print and study the parse trees for each sentence.  Do
    you agree with them?  Why or why not?

   + sentence 7 :: ``The black dogs are playing with the elf toy.''
   + sentence 8 :: ``The yellow dog slept in my pajamas.''
   + sentence 9 :: ``We will take two long rides in the country next week.''

12. [@12] <<fopc>> [[../../reading/blk_2nd_ed.pdf][Chapter 10 of BLK]] and [[http://www.nltk.org/howto/semantics.html][the semantics howto]] march one through the basics
   of applying the FOPC and the \lambda calculus to reifying the semantics of
   context-free sentences.  One of the practical difficulties in this
   approach is ensuring that the implementation of the universe of
   discourse (they call it the /domain of discourse/, same thing) actually
   covers the intended universe.

   To see this, let's use their =sem2.fcfg= grammar to parse the following
   sentences syntactically and semantically, and output the reification of
   the sentences into the FOPC and the \lambda calculus.  

   (HINT: be sure to 
   #+begin_src python :results output
   from nltk.sem import *
   #+end_src
   so you get all the parts and save yourself frustration!)  

   For each of the following sentences, parse them and print the sentence,
   its parse, and its semantics; and then explain the results you get and
   exactly how you would fix the problems encountered.

      + Suzie sees Noosa.
      + Fido barks.
      + Tess barks.




* Grading Scale

This homework is worth 15 points.  The grading
scale is:

| fraction correctly reviewed and answered | points awarded |
|------------------------------------------+----------------|
| \(\ge 0.95\)                             |             15 |
| 0.90 -- 0.94                             |             14 |
| 0.85 -- 0.89                             |             13 |
| 0.80 -- 0.94                             |             12 |
| 0.75 -- 0.79                             |             11 |
| 0.70 -- 0.74                             |             10 |
| 0.65 -- 0.69                             |              9 |
| 0.60 -- 0.64                             |              8 |
| 0.55 -- 0.59                             |              7 |
| 0.50 -- 0.54                             |              6 |
| 0.45 -- 0.49                             |              5 |
| 0.40 -- 0.44                             |              4 |
| 0.35 -- 0.39                             |              3 |
| 0.30 -- 0.34                             |              2 |
| 0.25 -- 0.29                             |              1 |
| \(< 0.25\)                               |              0 |




* Scoring


|     question | max pts | answer ok? |
|--------------+---------+------------|
|            1 |       1 |            |
|            2 |       1 |            |
|            3 |       1 |            |
|            4 |       1 |            |
|            5 |       1 |            |
|            6 |       2 |            |
|            7 |       1 |            |
|            8 |       1 |            |
|            9 |       1 |            |
|           10 |       1 |            |
|           11 |       2 |            |
|           12 |       2 |            |
|--------------+---------+------------|
|  total score |      15 |          0 |
|   percentage |         |          0 |
| total points |         |            |
#+TBLFM: @14$2=vsum(@I..@II)::@14$3=vsum(@I..@II)::@15$3=@-1/@-1$-1



